{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3d6aeb",
   "metadata": {
    "id": "6b3d6aeb"
   },
   "source": [
    "# Movie Clip Evidence Collector (YouTube)\n",
    "\n",
    "This notebook builds an **end-to-end** tool to:\n",
    "- Identify videos from specified YouTube channels\n",
    "- Collect metadata using the **YouTube Data API v3**\n",
    "- Preserve evidence artifacts (ToS-safe default):  \n",
    "  - API JSON (`video.json`)  \n",
    "  - Watch-page HTML snapshot (`watch.html`)  \n",
    "  - Thumbnail (`thumbnail.jpg`)  \n",
    "  - Integrity manifest with SHA-256 hashes (`manifest.json`)  \n",
    "- Output a formatted **Excel report** listing videos and evidence paths\n",
    "\n",
    "## Important note\n",
    "This notebook defaults to **ToS-safe evidence preservation** (metadata + snapshots + hashes) and does **not** download video streams. Full media capture is designed as an **optional plug-in** that should only be enabled when you have explicit authorization or a platform-approved mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3f037",
   "metadata": {
    "id": "eba3f037"
   },
   "source": [
    "A Google Cloud project with the **YouTube Data API v3** is enabled and an API key is created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1bd19a1",
   "metadata": {
    "id": "c1bd19a1"
   },
   "outputs": [],
   "source": [
    "!pip install -q google-api-python-client pandas openpyxl requests pyyaml beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5b9fd",
   "metadata": {
    "id": "b1a5b9fd"
   },
   "source": [
    "##Configuration\n",
    "\n",
    "We store configuration in a simple Python dict.\n",
    "(Can be switched to a `config.yaml` file later if desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce44c1a",
   "metadata": {
    "id": "9ce44c1a"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"api_key\": \"YOUR_YOUTUBE_API_KEY\",\n",
    "    \"channels\": [\n",
    "    \"@ScreenRantPlus\",\n",
    "    \"@ScreenRant\",\n",
    "    \"@MOVIECLIPS\",\n",
    "    \"@JoBloMovieClips\",\n",
    "    \"@movieclipstrailers\",\n",
    "    \"@rottentomatoestrailers\",\n",
    "    \"@WatchMojo\",\n",
    "    \"@topmovieclips5056\",\n",
    "    \"@FilmIsNow\",\n",
    "    \"@KinoCheckInternational\",\n",
    "    \"@IGNMovieTrailers\",\n",
    "    \"@FandangoMovieclips\",\n",
    "],\n",
    "    \"max_videos_per_channel\": 50,\n",
    "    \"output_dir\": \"evidence\",\n",
    "    \"report_dir\": \"reports\",\n",
    "    \"db_path\": \"state.db\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067872e",
   "metadata": {
    "id": "b067872e"
   },
   "source": [
    "## Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f93290b",
   "metadata": {
    "id": "0f93290b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, Alignment\n",
    "from openpyxl.utils import get_column_letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970941a",
   "metadata": {
    "id": "9970941a"
   },
   "source": [
    "Hashing & directory helpers\n",
    "\n",
    "Hashing artifacts to support **integrity** and basic **chain-of-custody**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b049f6b",
   "metadata": {
    "id": "2b049f6b"
   },
   "outputs": [],
   "source": [
    "def ensure_dirs(*paths: str) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def sha256_file(path: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def iso8601_duration_to_seconds(dur: str):\n",
    "    if not dur or not dur.startswith(\"PT\"):\n",
    "        return None\n",
    "    m = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", dur)\n",
    "    if not m:\n",
    "        return None\n",
    "    hours = int(m.group(1)) if m.group(1) else 0\n",
    "    minutes = int(m.group(2)) if m.group(2) else 0\n",
    "    seconds = int(m.group(3)) if m.group(3) else 0\n",
    "    return hours * 3600 + minutes * 60 + seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdb720",
   "metadata": {
    "id": "51fdb720"
   },
   "source": [
    "## SQLite State\n",
    "\n",
    "store processed videos to:\n",
    "- avoid duplicates\n",
    "- support incremental runs\n",
    "- track last_seen timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191d34cd",
   "metadata": {
    "id": "191d34cd"
   },
   "outputs": [],
   "source": [
    "def init_db(db_path: str) -> sqlite3.Connection:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\n",
    "        'CREATE TABLE IF NOT EXISTS videos ('\n",
    "        'video_id TEXT PRIMARY KEY,'\n",
    "        'channel_id TEXT,'\n",
    "        'channel_handle TEXT,'\n",
    "        'title TEXT,'\n",
    "        'published_at TEXT,'\n",
    "        'url TEXT,'\n",
    "        'risk_score REAL,'\n",
    "        'evidence_dir TEXT,'\n",
    "        'first_seen_at TEXT,'\n",
    "        'last_seen_at TEXT'\n",
    "        ')'\n",
    "    )\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def upsert_video(conn: sqlite3.Connection, row: tuple) -> None:\n",
    "    conn.execute(\n",
    "        'INSERT INTO videos(video_id, channel_id, channel_handle, title, published_at, url, risk_score, evidence_dir, first_seen_at, last_seen_at) '\n",
    "        'VALUES(?,?,?,?,?,?,?,?,?,?) '\n",
    "        'ON CONFLICT(video_id) DO UPDATE SET '\n",
    "        'last_seen_at=excluded.last_seen_at, '\n",
    "        'risk_score=excluded.risk_score, '\n",
    "        'evidence_dir=excluded.evidence_dir'\n",
    "    , row)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5e531",
   "metadata": {
    "id": "7ce5e531"
   },
   "source": [
    "## Detection Heuristics (Risk Scoring)\n",
    "\n",
    "- Keywords in title/description (clip, scene, ending, etc.)\n",
    "- Short-ish durations\n",
    "- Commercial indicators (sponsor, affiliate, promo)\n",
    "\n",
    "Later, it can upgrade to perceptual-hash matching against rights-owner reference frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0f3b75",
   "metadata": {
    "id": "1a0f3b75"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "KEYWORDS = [\"clip\", \"scene\", \"ending\", \"trailer\", \"deleted\", \"full scene\", \"best moments\", \"1080p\", \"4k\"]\n",
    "\n",
    "def compute_risk_score(title, description, duration_seconds):\n",
    "    text = f\"{title}\\n{description}\".lower()\n",
    "\n",
    "    words = set(re.findall(r\"[a-z0-9]+\", text))\n",
    "\n",
    "    score = 0.0\n",
    "    for k in [\"clip\", \"scene\", \"ending\", \"trailer\", \"deleted\", \"1080p\", \"4k\"]:\n",
    "        if k in words:\n",
    "            score += 2.0\n",
    "\n",
    "    # phrase keywords\n",
    "    if \"full scene\" in text:\n",
    "        score += 2.5\n",
    "    if \"best moments\" in text:\n",
    "        score += 2.0\n",
    "\n",
    "    if duration_seconds is not None:\n",
    "        if 10 <= duration_seconds <= 600:\n",
    "            score += 3.0\n",
    "        elif duration_seconds <= 1200:\n",
    "            score += 1.0\n",
    "\n",
    "    if any(x in text for x in [\"sponsor\", \"affiliate\", \"promo\"]):\n",
    "        score += 1.5\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ecf1b4",
   "metadata": {
    "id": "18ecf1b4"
   },
   "source": [
    "## Resolve channels and list videos\n",
    "\n",
    "We:\n",
    "1. Build a YouTube API client using the API key\n",
    "2. Resolve channel handles (like `@ScreenRantPlus`) into `channelId`\n",
    "3. List recent videos via the `search.list` endpoint\n",
    "4. Fetch details via `videos.list`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddc490cd",
   "metadata": {
    "id": "ddc490cd"
   },
   "outputs": [],
   "source": [
    "def build_youtube(api_key: str):\n",
    "    return build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "def resolve_channel_id(youtube, handle_or_query: str):\n",
    "    req = youtube.search().list(part=\"snippet\", q=handle_or_query, type=\"channel\", maxResults=1)\n",
    "    resp = req.execute()\n",
    "    items = resp.get(\"items\", [])\n",
    "    if not items:\n",
    "        return None\n",
    "    return items[0][\"snippet\"][\"channelId\"]\n",
    "\n",
    "def list_channel_videos(youtube, channel_id: str, max_results: int = 50):\n",
    "    videos = []\n",
    "    page_token = None\n",
    "    while True:\n",
    "        remaining = max_results - len(videos)\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        req = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            channelId=channel_id,\n",
    "            maxResults=min(50, remaining),\n",
    "            order=\"date\",\n",
    "            type=\"video\",\n",
    "            pageToken=page_token\n",
    "        )\n",
    "        resp = req.execute()\n",
    "        for it in resp.get(\"items\", []):\n",
    "            videos.append(it[\"id\"][\"videoId\"])\n",
    "            if len(videos) >= max_results:\n",
    "                return videos\n",
    "        page_token = resp.get(\"nextPageToken\")\n",
    "        if not page_token:\n",
    "            break\n",
    "    return videos\n",
    "\n",
    "def fetch_video_details(youtube, video_ids):\n",
    "    out = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        chunk = video_ids[i:i+50]\n",
    "        req = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=\",\".join(chunk),\n",
    "            maxResults=50\n",
    "        )\n",
    "        resp = req.execute()\n",
    "        out.extend(resp.get(\"items\", []))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37604a16",
   "metadata": {
    "id": "37604a16"
   },
   "source": [
    "## Evidence Preservation (ToS-safe default)\n",
    "\n",
    "For each video:\n",
    "- `video.json` (API response)\n",
    "- `watch.html` (watch-page snapshot)\n",
    "- `thumbnail.jpg` (best available)\n",
    "- `manifest.json` (hashes, sizes, timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842df041",
   "metadata": {
    "id": "842df041"
   },
   "outputs": [],
   "source": [
    "def preserve_evidence(video: dict, evidence_dir: str):\n",
    "    ensure_dirs(evidence_dir)\n",
    "\n",
    "    captured_at_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # Save API JSON\n",
    "    api_json_path = os.path.join(evidence_dir, \"video.json\")\n",
    "    with open(api_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(video, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save watch page HTML snapshot\n",
    "    video_id = video[\"id\"]\n",
    "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    html_path = os.path.join(evidence_dir, \"watch.html\")\n",
    "    r = requests.get(url, timeout=30, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    r.raise_for_status()\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "    # Download thumbnail\n",
    "    thumbs = video.get(\"snippet\", {}).get(\"thumbnails\", {})\n",
    "    thumb_url = None\n",
    "    for key in [\"maxres\", \"standard\", \"high\", \"medium\", \"default\"]:\n",
    "        if key in thumbs:\n",
    "            thumb_url = thumbs[key][\"url\"]\n",
    "            break\n",
    "\n",
    "    thumb_path = None\n",
    "    if thumb_url:\n",
    "        thumb_path = os.path.join(evidence_dir, \"thumbnail.jpg\")\n",
    "        img = requests.get(thumb_url, timeout=30)\n",
    "        img.raise_for_status()\n",
    "        with open(thumb_path, \"wb\") as f:\n",
    "            f.write(img.content)\n",
    "\n",
    "    # Manifest with hashes + timestamps\n",
    "    manifest = {\n",
    "        \"captured_at_utc\": captured_at_utc,\n",
    "        \"video_url\": url,\n",
    "        \"artifacts\": []\n",
    "    }\n",
    "    for p in [api_json_path, html_path, thumb_path]:\n",
    "        if p and os.path.exists(p):\n",
    "            manifest[\"artifacts\"].append({\n",
    "                \"path\": os.path.basename(p),\n",
    "                \"sha256\": sha256_file(p),\n",
    "                \"bytes\": os.path.getsize(p)\n",
    "            })\n",
    "\n",
    "    manifest_path = os.path.join(evidence_dir, \"manifest.json\")\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return {\n",
    "        \"video_url\": url,\n",
    "        \"evidence_dir\": evidence_dir,\n",
    "        \"manifest_path\": manifest_path,\n",
    "        \"captured_at_utc\": captured_at_utc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36236aae",
   "metadata": {
    "id": "36236aae"
   },
   "source": [
    "## Excel Reporting\n",
    "\n",
    "- channel handle + channelId\n",
    "- videoId + title + publish time\n",
    "- duration + stats\n",
    "- risk score\n",
    "- evidence folder + manifest path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9ab59b",
   "metadata": {
    "id": "2b9ab59b"
   },
   "outputs": [],
   "source": [
    "def write_excel(report_rows, out_path: str):\n",
    "    df = pd.DataFrame(report_rows)\n",
    "    ensure_dirs(os.path.dirname(out_path))\n",
    "    df.to_excel(out_path, index=False)\n",
    "\n",
    "    wb = load_workbook(out_path)\n",
    "    ws = wb.active\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "    header_font = Font(bold=True)\n",
    "    for cell in ws[1]:\n",
    "        cell.font = header_font\n",
    "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "\n",
    "    # Auto width (approx)\n",
    "    for col in range(1, ws.max_column + 1):\n",
    "        col_letter = get_column_letter(col)\n",
    "        max_len = 10\n",
    "        for row in range(1, ws.max_row + 1):\n",
    "            v = ws.cell(row=row, column=col).value\n",
    "            if v is not None:\n",
    "                max_len = max(max_len, min(80, len(str(v))))\n",
    "        ws.column_dimensions[col_letter].width = max_len + 2\n",
    "\n",
    "    wb.save(out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5f6eb",
   "metadata": {
    "id": "f3d5f6eb"
   },
   "source": [
    "## The Pipeline\n",
    "\n",
    "It will:\n",
    "- Resolve channels\n",
    "- Fetch latest videos\n",
    "- Preserve evidence into `CONFIG[\"output_dir\"]`\n",
    "- Save report into `CONFIG[\"report_dir\"]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02729d19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02729d19",
    "outputId": "11447d0c-e45b-4f50-b2e2-6231dfd1c53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Videos processed: 284\n",
      "[OK] Report written: reports/report_20260202_202530.xlsx\n",
      "[OK] Evidence root: evidence\n"
     ]
    }
   ],
   "source": [
    "def to_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return None\n",
    "def run_pipeline(cfg: dict):\n",
    "    if not cfg.get(\"api_key\") in cfg[\"api_key\"]:\n",
    "        raise ValueError(\"Please set CONFIG['api_key'] to your YouTube Data API key.\")\n",
    "\n",
    "    ensure_dirs(cfg[\"output_dir\"], cfg[\"report_dir\"])\n",
    "    conn = init_db(cfg[\"db_path\"])\n",
    "    youtube = build_youtube(cfg[\"api_key\"])\n",
    "\n",
    "    report = []\n",
    "    now_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for handle in cfg[\"channels\"]:\n",
    "        channel_id = resolve_channel_id(youtube, handle)\n",
    "        if not channel_id:\n",
    "            print(f\"[WARN] Could not resolve channel: {handle}\")\n",
    "            continue\n",
    "\n",
    "        video_ids = list_channel_videos(youtube, channel_id, cfg.get(\"max_videos_per_channel\", 50))\n",
    "        if not video_ids:\n",
    "            print(f\"[INFO] No videos found for {handle}\")\n",
    "            continue\n",
    "\n",
    "        videos = fetch_video_details(youtube, video_ids)\n",
    "\n",
    "        for v in videos:\n",
    "            sn = v.get(\"snippet\", {})\n",
    "            cd = v.get(\"contentDetails\", {})\n",
    "            st = v.get(\"statistics\", {})\n",
    "\n",
    "            view_count = to_int(st.get(\"viewCount\"))\n",
    "            like_count = to_int(st.get(\"likeCount\"))\n",
    "            comment_count = to_int(st.get(\"commentCount\"))\n",
    "\n",
    "            dur_s = iso8601_duration_to_seconds(cd.get(\"duration\"))\n",
    "            score = compute_risk_score(sn.get(\"title\", \"\"), sn.get(\"description\", \"\"), dur_s)\n",
    "\n",
    "            vid = v[\"id\"]\n",
    "            evidence_dir = os.path.join(cfg[\"output_dir\"], f\"channel_{channel_id}\", vid)\n",
    "            ev = preserve_evidence(v, evidence_dir)\n",
    "\n",
    "            url = ev[\"video_url\"]\n",
    "            row = (\n",
    "                vid, channel_id, handle, sn.get(\"title\",\"\"),\n",
    "                sn.get(\"publishedAt\",\"\"), url, score,\n",
    "                evidence_dir, now_utc, now_utc\n",
    "            )\n",
    "            upsert_video(conn, row)\n",
    "\n",
    "            report.append({\n",
    "                \"channel_handle\": handle,\n",
    "                \"channel_id\": channel_id,\n",
    "                \"video_id\": vid,\n",
    "                \"title\": sn.get(\"title\",\"\"),\n",
    "                \"published_at\": sn.get(\"publishedAt\",\"\"),\n",
    "                \"duration_seconds\": dur_s,\n",
    "                \"view_count\": view_count,\n",
    "                \"like_count\": like_count,\n",
    "                \"comment_count\": comment_count,\n",
    "                \"risk_score\": score,\n",
    "                \"video_url\": url,\n",
    "                \"evidence_dir\": evidence_dir,\n",
    "                \"manifest_path\": ev[\"manifest_path\"],\n",
    "                \"captured_at_utc\": ev[\"captured_at_utc\"]\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_xlsx = os.path.join(cfg[\"report_dir\"], f\"report_{ts}.xlsx\")\n",
    "    write_excel(report, out_xlsx)\n",
    "\n",
    "    print(f\"[OK] Videos processed: {len(report)}\")\n",
    "    print(f\"[OK] Report written: {out_xlsx}\")\n",
    "    print(f\"[OK] Evidence root: {cfg['output_dir']}\")\n",
    "\n",
    "    return out_xlsx, cfg[\"output_dir\"]\n",
    "\n",
    "out_xlsx, evidence_root = run_pipeline(CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VWoZKFmDmaEL",
   "metadata": {
    "id": "VWoZKFmDmaEL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
